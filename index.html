<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Invariance Through Inference</title>
    <meta name="description"
          content="We introduce a general approach, called Invariance Through Inference,
                for improving the test-time performance of an agent in deployment environments with unknown perceptual variations.
                Instead of producing invariant visual features through interpolation,
                invariance through inference turns adaptation at deployment-time into an unsupervised learning problem.
                This is achieved in practice by deploying a straightforward algorithm that tries to match the distribution of latent features
                to the agent's prior experience, without relying on paired data.
                Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to deployment-time rewards,
                including changes in camera poses and lighting conditions.
                Results are presented on challenging distractor control suite, a robotics environment with image-based observations.">

    <meta name="keywords" content="Deep Reinforcement Learning, Domain Adaptation, Generalization">
    <meta name="author" content="Takuma Yoneda <takuma@ttic.edu>">
    <meta property="og:title" content="Invariance Through Inference">
    <meta property="og:image" content="media/thumbnail.jpg">
    <meta name="twitter:creator" content="@takuma_ynd">
    <meta name="twitter:card" content="summary">
    <meta property="og:description"
          content="We introduce a general approach, called Invariance Through Inference,
                    for improving the test-time performance of an agent in deployment environments with unknown perceptual variations.
                    Instead of producing invariant visual features through interpolation,
                    invariance through inference turns adaptation at deployment-time into an unsupervised learning problem.
                    This is achieved in practice by deploying a straightforward algorithm that tries to match the distribution of latent features
                    to the agent's prior experience, without relying on paired data.
                    Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to deployment-time rewards,
                    including changes in camera poses and lighting conditions.
                    Results are presented on challenging distractor control suite, a robotics environment with image-based observations.">
    <link rel="stylesheet" href="./style.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>

</head>
<body>
<article>
    <section id="frontmatter">
        <h1>Invariance Through Inference</h1>
        <h2 id="authors" style="margin-bottom: 0;">Takuma Yoneda,<sup>1</sup> Ge Yang,<sup>2</sup>
            Matthew Walter,<sup>1</sup> Bradly Stadie<sup>1</sup>
        </h2>
        <h3 style="margin-top: 10px;"><sup>1</sup>TTI-Chicago, <sup>2</sup>MIT CSAIL </h3>
        <h3 id="links">
            <a href="https://github.com/takuma-yoneda/invariance-through-inference">CODE</a
            >|<a href="https://openreview.net/pdf?id=vXGcHthY6v">PAPER</a>
        </h3>
        <h2>Overview</h2>
        <p><b>Inverse Through Inference</b> (ITI) is a self-supervised adaptation method for a Reinforcement Learning agent.
            ITI adapts the encoder of an agent to a target domain without access to reward.</p>
        <p>
            <video autoplay muted loop playsinline width="60%" height="auto" class="center-wide">
                <source src="media/demo-video-noanimation.webm" type="video/webm">
            </video>
        </p>
        <!-- <iframe class="video" width="100%" height="338px" src="https://www.youtube.com/embed/KtvTt3U5bME?rel=0"
             frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
             allowfullscreen></iframe> -->
    </section>
    <h2 id="abstract">Abstract</h2>
    <p>
        We introduce a general approach, called Invariance Through Inference,
        for improving the test-time performance of an agent in deployment environments with unknown perceptual variations.
        Instead of producing invariant visual features through interpolation,
        invariance through inference turns adaptation at deployment-time into an unsupervised learning problem.
        This is achieved in practice by deploying a straightforward algorithm that tries to match the distribution of latent features
        to the agent's prior experience, without relying on paired data.
        Although simple, we show that this idea leads to surprising improvements on a variety of adaptation scenarios without access to deployment-time rewards,
        including changes in camera poses and lighting conditions.
        Results are presented on challenging distractor control suite, a robotics environment with image-based observations.
    </p>

    <h2 id="motivation">Motivation</h3>
    <p>
        Pixel-based RL agents are known to be brittle against distractions, due to its large shift in observation space.
        A typical approach to this issue is to apply <b>data augmentation</b>.
        This corresponds to expanding the support of the training distribution, as shown in the green circle below.
        <!-- SVEA, DrQ-v2 and SODA are among those methods. -->
    </p>
    <p>
        Training with augmented observations makes the agent more robust against distractions,
        however, as the target distribution (the pink circle below) goes far away from training,
        more and more augmentations becomes necessary, which becomes infeasible at some point.
        <img src="./media/motivation2.png" alt="motivation" class="center-narrow">
    </p>

    <p>
        When we have some knowledge of the target (test) domain, a better approach would be <b>domain adaptation</b>
        that adapt the agent to the target (test) domain.

        In this paper, we assume the target domain is accessible except for its reward, and
        propose <b>Invariance Through Inference</b> (ITI) that performs self-supervised domain adaptation.
        Specifically, ITI adapts an observation encoder
        so that the pretrained downstream policy \(\pi(a|z)\) can transfer to the target domain without modification.

        <!-- <img src="./motivation.png" alt="ITI-motivation" class="center"> -->

        <!-- The shift in observation space makes the corresponding latent space largely deviate.
             The task of ITI is to bring it back to align the latent structures, by adapting the encoder. -->
    </p>
    <p>
        We consider that it is <u>the large distribution shift in the latent space</u> that causes a poor performance in the target domain.
        Our approach attempts to <i>undo</i> this shift, by adapting the encoder based on two objectives: (1) <i>distribution matching</i> and (2) <i>dynamics consistency</i>.
    </p>

    <h2 id="method">Method</h2>
    <p>
        Given an agent pretrained in a source domain, a random policy collects transitions in the source domain.
        The observations are encoded with pretrained encoder, and the resulting <i>latent</i> transitions \((z_t, a_t, z_{t+1})\) are stored into source buffer.
        <!-- It should be noted that we encode observations and store <i>latent</i> transitions . -->
        <img src="./media/preprocess1.png" alt="ITI-preprocess" class="center-wide">
        Succeedingly, we pretrain dynamics networks \(C_\text{dyn}\) using samples from the buffer.
        \(C_\text{dyn}\) consists of forward dynamics network \(\hat{z}_{t+1} = C_\text{fwd}(z_t, a_t)\)
        and inverse dynamics network \(\hat{a}_t = C_\text{inv}(z_t, z_{t+1})\).
        We can think of this step as implicitly encoding the latent transition structure (somewhat like MDP) of the source domain into the weights of these networks.
    </p>
    <p>
        We also collect random transitions in the target domain. But this time we store transitions with raw observations \((o_t, a_t, o_{t+1})\).
        <img src="./media/preprocess2.png" alt="ITI-preprocess" width="60%" class="center">
    </p>
    <p>
        Once the preprocessing steps described above are completed, the main adaptation step begins.
        We use sample transitions from source and target buffer, and train encoder \(F\) (intialized to the pretrained weights) and discriminator \(D\) (initialized randomly).


        <!-- ITI adapts encoder following the two objectives: <i>distribution matching</i> and <i>dynamics consistency</i>. -->
        <!-- <img src="./method.png" alt="ITI" class="center"> -->
        <video autoplay muted loop playsinline class="center" poster="media/method.png">
            <source src="media/method2.webm" type="video/webm">
            <!-- <source src="media/method.mp4" type="video/mp4"> -->
        </video>
        <small>* You can download the static version <a href="https://example.com">here</a></small>

    </p>

    <h2 id="experiments">Experiments</h2>
    <p>
        To experiment our adaptation scheme on various type of target domains, we deployed our agents in DeepMind Control suite.
        Especially, we employed a modified version of DistractCS that provides <i>color</i>, <i>camera</i>, and <i>background distractions</i> to DeepMind Control suite.
        <img src="./media/intensity-example.png" alt="intensity-example" class="center">
    </p>
    <p>
        The plot below shows how much gain ITI can achieve on top of base policies.
        DrQ-v2 and SVEA are extensions of SAC that incorporates data augmentation.
        All of the methods are first trained in source domain (i.e., standard, non-distracting domain), and then directly deployed in taget domain (zero-shot).
        Zero-shot performance are shown as dashed lines below.
        As expected, the performance decreases as the intensity goes up.
        <img src="./media/main-result.png" alt="main-result" class="center-wide">
        On top of these, we applied ITI to adapt the encoders.
    </p>
    <p>
        <img src="./media/main-table.png" alt="main-table" class="center-wide">
    </p>


    <!-- <h2>BibTex</h2>
         <pre>@inproceedings{invr-thru-inf,
         title={Need to fill this after publishing the paper},
         author={Yang, Ge and Zhang, Amy and Morcos, Ari S. and Pineau, Joelle
         and Abbeel, Pieter and Calandra, Roberto},
         booktitle={Proceedings of The 2nd Annual Conference on Learning for Dynamics and Control},
         series={Proceedings of Machine Learning Research},
         pages={1-12},
         year={2020},
         volume={120},
         note={arXiv:2005.03648}
         }</pre> -->
</article>
</body>
</html>
